## 1 介绍
由于很多实践上的应用，包括文档分析，场景理解和机器人导航，还有图片复原，识别自然场景下的图片上的文字的已经引起计算机视觉社区的广泛的关注。虽然说之前的工作已经取得了显著的进步，不论是在文字检测方面还是文字识别方面，但对于多样性的文字模式和大量复杂的背景来说，是这仍然是一个巨大的挑战。

在这个自然场景的文字检测上的最简单的方法，就是将其划分成两个任务，包括文字检测和文字识别。深度学习的方法已经成为了主流。在文字检测上的通常一个卷积神经网络被使用的来从文字图片中提取特征图，然后呢，不同的解码器在这个区域上被使用。在文字识别方面，序列预测网络被接二连三应用在了文字识别的结果上。这导致的巨大的时间开销，尤其对于有大量的文字区域的图片来说。另外一个问题呢？是他忽略了，在文字检测和文字识别上的视觉线索的相关性。检测的网络不能被文字识别的结果调整，而且反之亦然。

在这篇文章里，我们提出，同时考虑文字检测和文字识别结果。这我们村为快速的方向文字识别系统，他可以被端到端的训练。和之前的两步的文字识别系统不同，我们的方法可以通过卷积神经网络学习到更多的普遍特征。说是可以被文字检测和文字识别所共享的，并且。这两项任务。相互监督是互补的。由于特征提取通常需要耗费大量的时间，我们将他的计算缩小到一个单个的检测网络，如图一所示。连接检测和识别的关键就是roirotate。它是通过在特征图上通过方向检测框来得到合适的特征的。

整个结构是在图2当中展示出来。特征图首先是由一个共享的卷积网络来提取出来。基于方向文字检测的全卷积网络分支是建立在特征图的基础上，他用来预测文本框的位置。roirotate模块提取从特征图上得到的检测结果区域的文字提取特征。然后那个提取特征，是被送到rnn循环卷积网络编码和ctc解码器当中进行识别。所有的网络的模块都是可以反向求导训练的，所以整个系统可以被端到端的训练。据我们所知这是第一个可以被端到端的训练的方向文字检测识别系统。我发现这个网络是可以很容易的被训练，不需要复杂的前处理和参数调整。
我们主要的贡献是在下面列出来这几点点。
我们对于方向文字识别，我们提出了一个端到端的训练架构。通过共享卷积特征网络可以进行同时的检测和识别，不需要太大的计算消耗，这时的它可以达到实时的速度。
我们提出了roirotate，这是一个新的可以反向求导模块，它是用来从卷积特征图中提取方向文字区域的。这个模块统一文字检测和识别到一个端到端的流程中。
这个方法呢，很轻松的在icdar这些数据集上。就能够取得比目前最好的方法还要更好的一个文字检测和识别的结果。







## 3 方法
我们这方法写一个端到端的可训练的框架。他在同一个场景同时的检测和识别所有的文字。它包含四个部分。共享卷积层，文字检测分支，roirotate模块和文字识别分支。

### 3.1 整体架构
整体架构如图2所示，检测分支和识别分支共享卷积层，共享的网络结构如图3所示。共享卷积网络骨架是resnet50。受到fpn的启发，我们将低层级的特征图和高层级的语意特征图连接起来。由共享卷积层产生的特征图的分辨率是原图的1/4大小。文字检测分支通过使用由共享卷积层产生稠密的每个像素对文字的预测。提取出来的方向文字区域之后，roirotate模块转换对应的共享的特征到保持原图长宽比的固定高度的表示上。最后，文字识别分支识别候选区域中的文字。cnn和lstm是用来编码文字序列信息的，后面接一个ctc解码器。文字识别分支的架构如表一所示。






### 3.2 文字检测分支
受到[53, 15]的启发，我们采用了一个全卷积网络作为文字检测器。由于在自然场景下存在许多的小的文字块，我们在共享卷积层将特征图的大小由原来的1/32扩大到原图的1/4大小。在提取特征之后，接个用来产生密集的像素文字预测的卷积层。第一个通道是用来计算每一个像素是正样本的可能性。和[53]类似，在原文字区域的收缩部分的像素被认为是正样本。对于每个正样本，后面的4个通道预测它到包含该像素的矩形框的上，下，左，右的距离。最后一个通道预测相对于矩形框的方向。在对这些正样本采用阈值过滤和非极大值抑制后，得到最后结果。
在我们的实验中，我们观察了许多近似于文字笔划的模式难以归类，比如fences, lattices,等. 我们采用OHEM方法来区分这些模式，这样也能同时解决分类不平衡问题。这个方法使得在ICDAR 2015数据集上有了f-measure的2%的提升。
检测分支的loss函数包含两项：文字分类项和矩形框回归项。文字归类项可以看作是对降采样得分图中的每个像素的分类的loss。只有原始文字区域的收缩部分区域是被认为是正样本，而在矩形框和文字区域之间的区域则是“不用理会的区域”，该区域不会对分类的loss函数产生影响。将在得分图中由OHEM得到的正样本的集合记为Ω,损失函数可以记为如下公式：

这里|.|是指集合元素的数量，H(px, p∗x)表示对于预测得分图px和二值标签p*x的交叉熵。
至于回归的loss，我们采用[52]中的IOU loss和[53]中的旋转角度loss，因为它们对变化的目标形状、尺度和方向有强的鲁棒性：

这里，IoU(Rx,R∗x)是预测的矩形框Rx和真实值R*x的IoU loss。第二项是旋转角度loss， θx和θx∗代表预测角度和真实角度。我们在实验中设置参数λθ 为10，最终的检测损失可以写成如下：

 λreg控制两个loss间的平衡，实验中设置为1.



## 4 实验
 我们在三个公开的标准中评估了我们的方法：ICDAR 2015, ICDAR 2017 MLT和ICDAR 2013，并在定位和识别上超过了目前最好的方法，所有的训练数据是都是可以公开得到的。

### 4.1 基准数据集
ICDAR 2015 是指ICDAR 2015挑战项目4这个比赛，它普遍用来作为带方向的场景文字定位任务。这个数据集包括1000张训练图片和500张测试图片。这些图片是在平时没有刻意注意的情况下用Google眼镜拍下的，所以文字可以有做任意的方向。对于文字spotting任务，在测试阶段，它提供了3个特定的单词的列表词汇集合：“强”， “弱”，“正常”来作为参照。“强”词汇表为每张图片提供了100个单词，它们是包括了所有出现在图片中的单词。“弱”词汇表则包括所有在测试集中出现的单词。而“正常”词汇表是一个90k的单词表。在训练时，我们首先在ICDAR 2017 MLT的训练和验证集的9000张图像上训练我们的模型，然后我们用1000张ICDAR 2015的训练图片和229张ICDAR 2013的训练图片来微调我们的模型。
ICDAR 2017 MLT是一个多语言的文字数据集，它饮食7200张训练图片，1800张验证图片和9000张测试图片，这个数据集是由9种语言的场景图片组成的，文字可以是任意方向，所以它是更多样和更加具有挑战性的。这个数据集没有文字spotting任务，所以我们只展示文字检测的结果。我们使用训练集和验证集来训练我们的模型。
ICDAR 2013 包含229张训练图片和233张测试图片，和ICDAR 2015类似，在spotting任务上，它也提供了“强”，“弱”，“正常”三个词表，和上面的数据集不同的是，它仅仅包含水平的文字。虽然我们的方法是为了有方向的文字设计的，在这个数据集上的结果也表明了我们的方法也同样适用于水平的文字。由于只有太少的训练图片，我们用9000张ICDAR 2017 MLT的训练和验证的图片来进行训练一个预训练模型，然后使用229张ICDAR 2013的训练图片来微调。

### 4.2 与两个步骤的方法相比较
与之前的将检测和识别分开为两个不相关的任务来相比较，我们的方法同时训练这两个任务，并且二者可以相互促进。为了验证这点，我们构建了一个两个步骤的系统，它是将检测和识别分开训练的。检测网络是通过在我们提出的网络中移除识别分支，相似的，构建识别网络时，检测分支也被移除。对于识别网络，从原图上截下的文字区域被作为训练数据，这与之前的识别方法是相似的。
如表2，3，4所示，我们提出的FOTS在文字定位任务上著地超过了“我们的检测”两步骤的方法，在文字spotting任务上超过了“我们的两步骤”方法。结果表明我们的联合训练策略可以更好地促进模型参数收敛。
FOTS在文字检测方面效果更好，因为它的识别部分可以帮助监督网络来学习字符细节层面的特征。为了分析细节，我们总结了四种普通的在文字检测方面的问题，缺失：缺失一些文字区域，错误：错误地识别一些非文字区域为文字，分割：错误地将整个文字区域分割为几个单独的区域，合并：错误地合并几个单独的文字区域在一起。如图5所示，与“我们的检测”方法相比FOTS极大减少了这四类错误。特别地，“我们的检测”方法整体的文字区域特征，而不是字符级别的特征，所以这种方法在当文字区域内部有巨大变化，或文字与背景有类似的样式等情况下，表现不是特别好。由于文字识别监督模型更专注于考虑字符的细节，FOTS可以学习在一个单词间的有不同样式的不同字符的语义信息。它也增强了在在字符和背景有相似样式时的区分。如图5所示，对于“缺失”情况，由于文字和背景颜色是相似的，“我们的检测”方法丢失了文字区域。对于“错误”情况，由于背景区域有“类似于文字”的样式（比如有巨大反差的重复的结构条纹），“我们的检测”方法错误地将背景识别为文字，而FOTS在结合训练识别loss后避免了这种错误，它考虑了文字区域内的细节。对于“分割”情况，由于左右两侧的文字有不同的颜色，“我们的检测”方法分割一个文字区域为两个，而FOTS预测这个区域为一整个区域，因为这个文字区域有相似的字符样式。对于“合并”情况，由于两个相邻的文字区域挨得很近且有相似的样式，“我们的检测“方法错误地将它们合并在一起，而FOTS利用识别出来的字符级别的特征信息来区分了这两个单词。

### 4.3 与最新的方法相比的结果
在这个部分，我们比较了FOTS与最新的方法的结果，如表2，3，4所示，我们的方法在所有数据集大大地提高了结果。由于ICDAR 2017 MLT 没有文字spotting任务，我们仅仅展示出它的文字检测结果。所有的ICDAR 2013数据都被使用水平的框来标注，因为它们大多数都是略微倾斜的，我们的模型是用ICDAR 2017 MLT数据预训练的，它也可以预测有方向的文字区域。我们的最终文字spotting结果保持方向预测的表现，并且由于评价标准的限制，我们的检测结果为网络预测框的最小水平外接矩形。值得一提的是，在ICDAR 2015文字spotting任务中，我们的方法在F-measure上，比之前的最好的方法提升了15%。
在单尺度测试时，对于ICDAR 2015,ICDAR 2017 MLT 和ICDAR 2013,FOTS将输入图像的长的一边resize到2240，1280，920，来取得最好的结果，我们使用3-5个尺度来做一个多尺度的测试。
### 4.4 速度和模型大小
如表5所示，受益于我们的卷积共享策略，FOTS相比于其它的单独的文字检测网络来说，可以同时检测和识别，并且不需要增加太多的计算和存储消耗。（7.5fps vs 7.8fps， 22.0fps vs 23.9 fps），并且它几乎两倍快于“我们的两步骤”方法（7.5fps vs 3.7fps， 22.0fps vs11.2 fps）。结果，我们的方法在保持实时速度上取得了最好的结果。
所有的方法都是在ICDAR 2015和ICDAR 2013I测试集上。这些数据集有68种文字识别标签，并且我们所有的测试图片和计算平均速度。对于ICDAR 2015,FOTS使用2240 * 1260大小的图片来检测，用32像素高度的截下的文字块来识别。为了取得实时的速度，"FOTS RT" 替换了ResNet-50为ResNet-34，并使用1280 * 720大小图片作为输入。所有的结果在表5中展示，是用Caffe的修改版本和TITAN-Xp GPU来实现的。